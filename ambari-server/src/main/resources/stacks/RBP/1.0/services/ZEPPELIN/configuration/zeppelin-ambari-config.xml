<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<!--
   Licensed to the Apache Software Foundation (ASF) under one or more
   contributor license agreements.  See the NOTICE file distributed with
   this work for additional information regarding copyright ownership.
   The ASF licenses this file to You under the Apache License, Version 2.0
   (the "License"); you may not use this file except in compliance with
   the License.  You may obtain a copy of the License at

       http://www.apache.org/licenses/LICENSE-2.0

   Unless required by applicable law or agreed to in writing, software
   distributed under the License is distributed on an "AS IS" BASIS,
   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   See the License for the specific language governing permissions and
   limitations under the License.
-->
 
<configuration>

  <!-- params for service -->
  <property>
    <name>zeppelin.setup.prebuilt</name>
    <value>true</value>
    <description>If true, will download previously built package (instead of building from source). To compile from source instead, set to false.</description>
  </property>

  <property>
    <name>zeppelin.executor.mem</name>
    <value>512m</value>
    <description>Executor memory to use (e.g. 512m or 1g)</description>
  </property> 

  <property>
    <name>zeppelin.executor.instances</name>
    <value>2</value>
    <description>Number of executor instances to use (e.g. 2)</description>
  </property> 


  <!-- <property>
    <name>zeppelin.spark.version</name>
    <value>1.4</value>
    <description>Version of Spark installed in location specified in SPARK_HOME. Default with HDP 2.3.2 is 1.4.1, but can also be set to 1.5 or 1.3 (if you manually installed Spark 1.3 or 1.4)</description>
  </property> -->

  <property>
    <name>zeppelin.spark.jar.dir</name>
    <value>hdfs:///apps/zeppelin</value>
    <description>Shared location where zeppelin spark jar will be copied to. Should be accesible by all cluster nodes</description>
  </property> 
  
  <property>
    <name>zeppelin.install.dir</name>
    <value>/opt</value>
    <description>Local dir where to install component. incubator-zeppelin folder will be created as a subdir of this dir e.g. /opt/incubator-zeppelin</description>
  </property> 

  <property>
    <name>zeppelin.setup.view</name>
    <value>true</value>
    <description>Whether the Zeppelin view should be compiled and sample notebooks downloaded. Set to false if cluster does not have internet access</description>
  </property> 

  <property>
    <name>zeppelin.temp.file</name>
    <value>/tmp/zeppelin.tar.gz</value>
    <description>Temporary file where pre-built package will be downloaded to. If your env has limited space under /tmp, change this to different location. In this case you must ensure that the zeppelin user must be able to write to this location.</description>
  </property>

  <property>
    <name>zeppelin.host.publicname</name>
    <value> </value>
    <description>This is used to setup the Ambari view for Zeppelin. Set this to the public host/IP of zeppelin node (which must must be reachable from your local machine). If installing on sandbox (or local VM), change this to the IP address of VM. 
    If installing on cloud, set this to public name/IP of zeppelin node. Alternatively, if you already have a local hosts file entry for the internal hostname of the zeppelin node (e.g. sandbox.hortonworks.com), you can leave this empty - it will default to internal hostname</description>
  </property>

  <property>
    <name>spark.home</name>
    <value>/usr/hdp/current/spark-client/</value>
    <description>Spark home directory. Defaults to the Spark that comes with HDP (e.g. 1.3.1 with HDP 2.3.0 or 1.4.1 with HDP 2.3.2). To point Zeppelin to different Spark build, change this to location of where you downloaded Spark to (e.g./home/zeppelin/spark-1.5.0-bin-hadoop2.6) </description>
  </property>
  
  <property>
    <name>zeppelin.install_python_packages</name>
    <value>false</value>
    <description>(Optional) (CentOS only) Whether to install python packages for pyspark. Installs numpy scipy pandas scikit-learn</description>
  </property>
  
    
</configuration>  
